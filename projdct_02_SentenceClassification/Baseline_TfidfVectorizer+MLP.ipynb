{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [문장 유형 분류 AI 경진대회](https://dacon.io/competitions/official/236037/overview/description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Info.\n",
    "\n",
    "+ train.csv [파일]\n",
    "    - ID : 샘플 문장 별 고유 ID\n",
    "    - 문장 : 샘플 별 한개의 문장\n",
    "    - 유형 : 문장의 유형 (사실형, 추론형, 대화형, 예측형)\n",
    "    - 극성 : 문장의 극성 (긍정, 부정, 미정)\n",
    "    - 확실성 : 문장의 확실성 (확실, 불확실)\n",
    "    - label : 문장 별 유형, 극성, 시제, 확실성에 대한 Class (총 72개 종류의 Class 존재)  \n",
    "    예시) 사실형-긍정-현재-확실\n",
    "\n",
    "\n",
    "- test.csv [파일]\n",
    "    + ID : 샘플 문장 별 고유 ID\n",
    "    + 문장 : 샘플 별 한개의 문장\n",
    "\n",
    "\n",
    "+ sample_submission.csv [파일] - 제출 양식\n",
    "    - ID : 샘플 문장 별 고유 ID\n",
    "    - label : 예측한 문장 별 유형, 극성, 시제, 확실성에 대한 Class  \n",
    "    예시) 사실형-긍정-현재-확실\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title = \"T00_Baseline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01_Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pytorch_gpu/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:1.13.0.dev20220913\n",
      "MPS 장치를 지원하도록 build 되었는지: True\n",
      "MPS 장치가 사용 가능한지: True\n",
      "macOS-12.5-arm64-arm-64bit\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "print (f\"PyTorch version:{torch.__version__}\") # 1.12.1 이상\n",
    "print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\") # True 여야 합니다.\n",
    "print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\") # True 여야 합니다.\n",
    "!python -c 'import platform;print(platform.platform())'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02_Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS': 10,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'BATCH_SIZE': 256,\n",
    "    'SEED': 190813\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03_Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04_Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./01_data/train.csv')\n",
    "test = pd.read_csv('./01_data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05_Train / Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.2, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06_Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A_문장(Text) 벡터화 via TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13232, 9295) (3309, 9295) (7090, 9295)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=4, analyzer='word', ngram_range=(1, 2))\n",
    "vectorizer.fit(np.array(train['문장']))\n",
    "\n",
    "train_vec = vectorizer.transform(train['문장'])\n",
    "val_vec = vectorizer.transform(val['문장'])\n",
    "test_vec = vectorizer.transform(test['문장'])\n",
    "\n",
    "print(train_vec.shape, val_vec.shape, test_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B_Label Encoding (`유형`, `극성`, `시제`, `확실성`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_le = preprocessing.LabelEncoder()\n",
    "train['유형'] = type_le.fit_transform(train['유형'].values)\n",
    "val['유형'] = type_le.transform(val['유형'].values)\n",
    "\n",
    "polarity_le = preprocessing.LabelEncoder()\n",
    "train['극성'] = type_le.fit_transform(train['극성'].values)\n",
    "val['극성'] = type_le.transform(val['극성'].values)\n",
    "\n",
    "tense_le = preprocessing.LabelEncoder()\n",
    "train['시제'] = type_le.fit_transform(train['시제'].values)\n",
    "val['시제'] = type_le.transform(val['시제'].values)\n",
    "\n",
    "certainty_le = preprocessing.LabelEncoder()\n",
    "train['확실성'] = type_le.fit_transform(train['확실성'].values)\n",
    "val['확실성'] = type_le.transform(val['확실성'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type = train['유형'].values\n",
    "train_polarity = train['극성'].values\n",
    "train_tense = train['시제'].values\n",
    "train_certainty = train['확실성'].values\n",
    "\n",
    "train_labels = {\n",
    "    'type': train_type,\n",
    "    'polarity': train_polarity,\n",
    "    'tense': train_tense,\n",
    "    'certainty': train_certainty\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_type = val['유형'].values\n",
    "val_polarity = val['극성'].values\n",
    "val_tense = val['시제'].values\n",
    "val_certainty = val['확실성'].values\n",
    "\n",
    "val_labels = {\n",
    "    'type': val_type,\n",
    "    'polarity': val_polarity,\n",
    "    'tense': val_tense,\n",
    "    'certainty': val_certainty\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07_Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, st_vec, st_labels):\n",
    "        self.st_vec = st_vec\n",
    "        self.st_labels = st_labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        st_vector = torch.FloatTensor(self.st_vec[index].toarray()).squeeze(0)\n",
    "        if self.st_labels is not None:\n",
    "            st_type = self.st_labels['type'][index]\n",
    "            st_polarity = self.st_labels['polarity'][index]\n",
    "            st_tense = self.st_labels['tense'][index]\n",
    "            st_certainty = self.st_labels['certainty'][index]\n",
    "            return st_vector, st_type, st_polarity, st_tense, st_certainty\n",
    "        else:\n",
    "            return st_vector\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.st_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_vec, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_vec, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08_Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_dim=9351):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.feature_extract = nn.Sequential(\n",
    "            nn.Linear(in_features=input_dim, out_features=1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.type_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=4),\n",
    "        )\n",
    "        self.polarity_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=3)\n",
    "        )\n",
    "        self.tense_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=3)\n",
    "        )\n",
    "        self.certainty_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # 문장 유형, 극성, 시제, 확실성을 각각 분류\n",
    "        type_output = self.type_classifier(x)\n",
    "        polarity_output = self.polarity_classifier(x)\n",
    "        tense_output = self.tense_classifier(x)\n",
    "        certainty_output = self.certainty_classifier(x)\n",
    "        return type_output, polarity_output, tense_output, certainty_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = {\n",
    "        'type': nn.CrossEntropyLoss().to(device),\n",
    "        'polarity': nn.CrossEntropyLoss().to(device),\n",
    "        'tense': nn.CrossEntropyLoss().to(device),\n",
    "        'certainty': nn.CrossEntropyLoss().to(device)\n",
    "    }\n",
    "    \n",
    "    best_loss = 999999\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for sentence, type_label, polarity_label, tense_label, certainty_label in tqdm(iter(train_loader)):\n",
    "            sentence = sentence.to(device)\n",
    "            type_label = type_label.to(device)\n",
    "            polarity_label = polarity_label.to(device)\n",
    "            tense_label = tense_label.to(device)\n",
    "            certainty_label = certainty_label.to(device)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            type_logit, polarity_logit, tense_logit, certainty_logit = model(sentence)\n",
    "            \n",
    "            loss = 0.25 * criterion['type'](type_logit, type_label) + \\\n",
    "                0.25 * criterion['polarity'](polarity_logit, type_label) + \\\n",
    "                0.25 * criterion['tense'](tense_logit, type_label) + \\\n",
    "                0.25 * criterion['certainty'](certainty_logit, type_label)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        val_loss, val_type_f1, val_polarity_f1, val_tense_f1, val_certainty_f1 = validation(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}]   Train Loss : [{np.mean(train_loss):.5f}]   Val Loss : [{val_loss:.5f}] 유형 F1: [{val_type_f1:.5f}] 극성 F1 : [{val_polarity_f1:.5f}] 시제 F1 : [{val_tense_f1:.5f}] 확실성 F1 : [{val_certainty_f1:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "        \n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    \n",
    "    type_preds, polarity_preds, tense_preds, certainty_preds = [], [], [], []\n",
    "    type_labels, polarity_labels, tense_labels, certainty_labels = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentence, type_label, polarity_label, tense_label, certainty_label in tqdm(iter(val_loader)):\n",
    "            sentence = sentence.to(device)\n",
    "            type_label = type_label.to(device)\n",
    "            polarity_label = polarity_label.to(device)\n",
    "            tense_label = tense_label.to(device)\n",
    "            certainty_label = certainty_label.to(device)\n",
    "            \n",
    "            type_logit, polarity_logit, tense_logit, certainty_logit = model(sentence)\n",
    "            \n",
    "            loss = 0.25 * criterion['type'](type_logit, type_label) + \\\n",
    "                0.25 * criterion['polarity'](polarity_logit, type_label) + \\\n",
    "                0.25 * criterion['tense'](tense_logit, type_label) + \\\n",
    "                0.25 * criterion['certainty'](certainty_logit, type_label)\n",
    "                \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            type_preds += type_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            type_labels += type_label.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            polarity_preds += polarity_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            polarity_labels += polarity_label.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            tense_preds += tense_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            tense_labels += tense_label.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            certainty_preds += certainty_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            certainty_labels += certainty_label.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    type_f1 = f1_score(type_labels, type_preds, average='weighted')\n",
    "    polarity_f1 = f1_score(polarity_labels, polarity_preds, average='weighted')\n",
    "    tense_f1 = f1_score(tense_labels, tense_preds, average='weighted')\n",
    "    certainty_f1 = f1_score(certainty_labels, certainty_preds, average='weighted')\n",
    "    \n",
    "    return np.mean(val_loss), type_f1, polarity_f1, tense_f1, certainty_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09_Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "model = BaseModel()\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG['LEARNING_RATE'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10_Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_vec, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    type_preds, polarity_preds, tense_preds, certainty_preds = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentence in tqdm(test_loader):\n",
    "            sentence = sentence.to(device)\n",
    "            \n",
    "            type_logit, polarity_logit, tense_logit, certainty_logit = model(sentence)\n",
    "            \n",
    "            type_preds += type_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            polarity_preds += polarity_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            tense_preds += tense_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            certainty_preds += certainty_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "        \n",
    "    return type_preds, polarity_preds, tense_preds, certainty_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_preds, polarity_preds, tense_preds, certainty_preds = inference(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_preds = type_le.inverse_transform(type_preds)\n",
    "polarity_preds = polarity_le.inverse_transform(polarity_preds)\n",
    "tense_preds = tense_le.inverse_transform(tense_preds)\n",
    "certainty_preds = certainty_le.inverse_transform(certainty_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for type_pred, polarity_pred, tense_pred, certainty_pred in zip(type_preds, polarity_preds, tense_preds, certainty_preds):\n",
    "    predictions.append(type_pred+'-'+polarity_pred+'-'+tense_pred+'-'+certainty_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11_Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./01_data/sample_submission.csv')\n",
    "submit['label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./02_submission/{test_title}_sumbit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a798a8cd8beadc83dc19adf1652df11aa0685d59b30007e08ffc64b33b3f407e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
